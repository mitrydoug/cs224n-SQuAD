# Word Games (Part Deux)

# design changes
BiDAF attention
Modeling layer as in paper (1 LSTM layer hiddens to start and to 2nd LSTM for end)
    uses concatenation of attention layer output as well

